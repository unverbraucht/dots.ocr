# Modern ROCm Dockerfile for DotsOCR with AMD GPUs
FROM rocm/pytorch:rocm6.4.3_ubuntu24.04_py3.12_pytorch_release_2.6.0

# Update system and install dependencies
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install required Python packages
RUN pip3 install --no-cache-dir \
    transformers==4.51.3 \
    accelerate \
    bitsandbytes \
    flash-attn --no-build-isolation || echo "Flash attention installation failed, continuing without it" \
    flask \
    pillow \
    requests

# Set working directory
WORKDIR /workspace

# ROCm environment variables
ENV ROCM_PATH=/opt/rocm
ENV HIP_VISIBLE_DEVICES=0
ENV HSA_OVERRIDE_GFX_VERSION=10.3.0
ENV PYTORCH_ROCM_ARCH="gfx900;gfx906;gfx908;gfx90a;gfx1030;gfx1100;gfx1101;gfx1102"

# Create model weights directory
RUN mkdir -p /workspace/weights/DotsOCR

# Create initialization script
COPY <<EOF /workspace/init_model.sh
#!/bin/bash
set -e

echo "Initializing DotsOCR for AMD GPU..."

# Create proper __init__.py if it doesn't exist
if [ ! -f "/workspace/weights/DotsOCR/__init__.py" ]; then
    cat > /workspace/weights/DotsOCR/__init__.py << 'PYEOF'
# DotsOCR Package
from . import modeling_dots_ocr
from . import modeling_dots_vision
from . import configuration_dots

# Note: vLLM version may not be needed for HuggingFace backend
try:
    from . import modeling_dots_ocr_vllm
    __all__ = ['modeling_dots_ocr_vllm', 'modeling_dots_ocr', 'modeling_dots_vision', 'configuration_dots']
except ImportError:
    __all__ = ['modeling_dots_ocr', 'modeling_dots_vision', 'configuration_dots']
PYEOF
    echo "DotsOCR __init__.py created successfully"
else
    echo "DotsOCR __init__.py already exists, skipping creation"
fi

# Verify ROCm installation
python3 -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'ROCm available: {torch.cuda.is_available()}'); print(f'ROCm version: {torch.version.hip if hasattr(torch.version, "hip") else "N/A"}')"

if [ -f "/workspace/weights/DotsOCR/config.json" ]; then
    echo "Model configuration found"
else
    echo "WARNING: Model weights may not be properly mounted"
fi

echo "Initialization complete!"
EOF

RUN chmod +x /workspace/init_model.sh

# Set Python path
ENV PYTHONPATH=/workspace/weights:$PYTHONPATH

# Create HuggingFace-based server optimized for AMD GPUs
COPY <<EOF /workspace/amd_server.py
#!/usr/bin/env python3
import torch
import sys
import os
sys.path.append('/workspace/weights')

from transformers import AutoModel, AutoProcessor
from flask import Flask, request, jsonify
import base64
from PIL import Image
import io
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Global variables for model and processor
model = None
processor = None
device = None

def initialize_model():
    global model, processor, device
    
    # Check device availability
    if torch.cuda.is_available():
        device = "cuda"
        logger.info(f"Using AMD GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        device = "cpu"
        logger.warning("CUDA/ROCm not available, falling back to CPU")
    
    model_path = "/workspace/weights/DotsOCR"
    
    try:
        logger.info(f"Loading DotsOCR model from {model_path}...")
        
        # Load with optimizations for AMD GPUs
        model = AutoModel.from_pretrained(
            model_path, 
            trust_remote_code=True,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            low_cpu_mem_usage=True
        )
        
        if device == "cuda" and model.device.type == "cpu":
            model = model.to(device)
        
        processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        
        logger.info(f"Model loaded successfully on {device}")
        return True
        
    except Exception as e:
        logger.error(f"Model loading failed: {e}")
        return False

@app.route('/health', methods=['GET'])
def health():
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "gpu_name": torch.cuda.get_device_name(0),
            "gpu_memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB",
            "gpu_memory_allocated": f"{torch.cuda.memory_allocated(0) / 1024**3:.1f} GB"
        }
    
    return jsonify({
        "status": "ok", 
        "device": device,
        "model_loaded": model is not None,
        "pytorch_version": torch.__version__,
        "rocm_version": getattr(torch.version, 'hip', 'N/A'),
        **gpu_info
    })

@app.route('/parse', methods=['POST'])
def parse_document():
    if model is None or processor is None:
        return jsonify({"error": "Model not loaded"}), 500
    
    try:
        data = request.get_json()
        if 'image' not in data:
            return jsonify({"error": "No image provided"}), 400
            
        # Decode base64 image
        image_data = base64.b64decode(data['image'])
        image = Image.open(io.BytesIO(image_data)).convert('RGB')
        
        # Process with DotsOCR
        logger.info(f"Processing image of size {image.size}")
        inputs = processor(image, return_tensors="pt")
        
        if device == "cuda":
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Generate with memory-efficient settings
        with torch.no_grad():
            if device == "cuda":
                with torch.cuda.amp.autocast():  # Use mixed precision for AMD GPUs
                    outputs = model.generate(
                        **inputs, 
                        max_new_tokens=data.get('max_tokens', 1024),
                        do_sample=False,
                        pad_token_id=processor.tokenizer.eos_token_id
                    )
            else:
                outputs = model.generate(
                    **inputs, 
                    max_new_tokens=data.get('max_tokens', 1024),
                    do_sample=False,
                    pad_token_id=processor.tokenizer.eos_token_id
                )
        
        result = processor.decode(outputs[0], skip_special_tokens=True)
        
        logger.info("Document processed successfully")
        return jsonify({"result": result})
    
    except Exception as e:
        logger.error(f"Processing error: {e}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    logger.info("Starting DotsOCR AMD GPU server...")
    
    if initialize_model():
        logger.info("Server starting on port 8000")
        app.run(host='0.0.0.0', port=8000, debug=False)
    else:
        logger.error("Failed to initialize model, exiting")
        sys.exit(1)
EOF

RUN chmod +x /workspace/amd_server.py

# Startup script for ROCm
COPY <<EOF /workspace/start_server_rocm.sh
#!/bin/bash
set -e

echo "=== DotsOCR AMD GPU Server Startup ==="

# Initialize model and environment
/workspace/init_model.sh

echo "Starting DotsOCR server optimized for AMD GPUs..."
python3 /workspace/amd_server.py
EOF

RUN chmod +x /workspace/start_server_rocm.sh

EXPOSE 8000
ENTRYPOINT ["/workspace/start_server_rocm.sh"]
