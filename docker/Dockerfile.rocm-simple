# Alternative: Using official ROCm runtime with manual PyTorch installation
FROM rocm/rocm-runtime:6.4.3-ubuntu24.04

# Install Python and pip
RUN apt-get update && apt-get install -y \
    python3.12 \
    python3.12-pip \
    python3.12-dev \
    git \
    wget \
    curl \
    && ln -sf /usr/bin/python3.12 /usr/bin/python3 \
    && ln -sf /usr/bin/python3.12 /usr/bin/python \
    && rm -rf /var/lib/apt/lists/*

# Install PyTorch with ROCm support
RUN pip3 install --no-cache-dir \
    torch==2.6.0+rocm6.4 \
    torchvision==0.21.0+rocm6.4 \
    torchaudio==2.6.0+rocm6.4 \
    --index-url https://download.pytorch.org/whl/rocm6.4

# Install other requirements
RUN pip3 install --no-cache-dir \
    transformers==4.51.3 \
    accelerate \
    flask \
    pillow \
    requests \
    numpy

# Set working directory
WORKDIR /workspace

# ROCm environment setup
ENV ROCM_PATH=/opt/rocm
ENV PATH=$ROCM_PATH/bin:$PATH
ENV LD_LIBRARY_PATH=$ROCM_PATH/lib:$LD_LIBRARY_PATH
ENV HIP_VISIBLE_DEVICES=0
ENV HSA_OVERRIDE_GFX_VERSION=10.3.0

# Create directories
RUN mkdir -p /workspace/weights/DotsOCR

# Copy the server script from the previous Dockerfile
COPY <<EOF /workspace/simple_amd_server.py
#!/usr/bin/env python3
import torch
import sys
sys.path.append('/workspace/weights')

print(f"PyTorch version: {torch.__version__}")
print(f"ROCm available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"Device name: {torch.cuda.get_device_name(0)}")
    print(f"Device count: {torch.cuda.device_count()}")

from flask import Flask, jsonify

app = Flask(__name__)

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        "status": "ok",
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "pytorch_version": torch.__version__,
        "rocm_available": torch.cuda.is_available()
    })

@app.route('/test', methods=['GET'])
def test():
    try:
        # Simple tensor operation test
        if torch.cuda.is_available():
            x = torch.randn(10, 10).cuda()
            y = x @ x.T
            result = "AMD GPU computation successful"
        else:
            x = torch.randn(10, 10)
            y = x @ x.T
            result = "CPU computation successful"
        
        return jsonify({"test_result": result, "tensor_shape": list(y.shape)})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000, debug=True)
EOF

RUN chmod +x /workspace/simple_amd_server.py

EXPOSE 8000
CMD ["python3", "/workspace/simple_amd_server.py"]
